---
title: Singular Value Decomposition (SVD)
comments: true
tags: 
- SVD

toc: true
toc_label: "Helpers"
toc_icon: "cogs"
---

- Drawbacks: slow, computationally expensive, missing data에 대한 care 필요 (whereas Gradient descent approach (convex optimization method)is much faster and deals well with missing data.)

- 기하학적 의미: 직교하는 벡터 집합에 대해서, 선형 변환 후에 그 크기는 변하지만 여전히 직교할 수 있게 되는 그 직교 집합은 무엇인가? 그리고 선형 변환 후의 결과는 무엇인가?

- $$A=U\Sigma V^T$$

- Singular Value $$\sigma$$ 어떻게 구할래?
  - power iteration 방법
  - $$A^TA$$ 의 eigenvalues 의 square root

- $$A^+$$ 는 A의 PseudoInverse 인데 모든 값이 양수여야 해서 + 로 적음
- $$A^+ \vec {x}$$의 근사치를 구할 수 있다면 (상대적으로 작은 $$\sigma$$ 들은 0으로 truncated) 몇몇의 가장 작은 $$\sigma$$를 구할 수 있음.     
- 그러면 완전히 $$A^+$$를 찾기보단 $$A^+ \vec {x}$$ 로 부터 그 $$\sigma$$들을 truncated 하는게 더 나음.
- A가 넓은 범위의 singular values를 가지는 경우에 정확할 수 있음. 그래서 Full $$A^+$$를 구하거나 저장하는거 보다 더 괜찮다.
-  $$\tilde A \equiv U\tilde {\Sigma}V^T$$ $$\tilde A$$는 작은 $$\sigma$$들을 truncated한 근사치


왜 Scikit-Learn은 PCA계산에서 SVD를 사용하는 것일까? 

그 이유는 정확히는 모르겠지만 stackoverflow에서 확인할 수 있듯이, eigenvalue-decomposition에서는 공분산 행렬을 메모리상에 가지고 있어야하는 반면 SVD는 공분산 행렬을 따로 메모리에 저장할 필요가 없으므로 효율적이기 때문이다.

출처: https://excelsior-cjh.tistory.com/167 [EXCELSIOR]
