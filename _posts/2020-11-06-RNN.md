---
title: RNN
comments: true
tags: [RNN]

use_math: true
---
특징: CNN은 grid 모양의 데이터에 특화되있다면 RNN은 sequential 데이터(대표적으로 시계열)에 특화되어 있음     

그래서 가장 중요한거는 **parameter sharing**.    

>(e.g.) I went to Nepal in 2009.     
>      In 2009, I went to Nepal.     

연도에 대해서 두 문장의 "2009"는 서로 다른 position에 위치함.    

이 문제를 해결하기 위해     
- 기존 "뉴런" 기반의 FC layer는 "갈라진(separate) 파라미터"들을 가지고 있다. 그래서 문장의 각 포지션마다 "모든 언어 규칙"을 따로따로 배워야할 필요가 있다.     
> parameter 개수 = Weight Matrix 크기 * kernel 크기 + bias 크기 (뉴런)

- 그래서, CNN안에 cell(뉴런)의 가중치는 time step에 대해 독립적이다.     
 반대로, RNN은 시간의 변화(step?)에도 같은 가중치 변수를 가지고 공유한다. 서로 다른 input들은 시간을 통해서 서로 인과관계를 가진다.
그럼 Markov random field랑 무엇이 다른가?
- Node in RNN : 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할, cell이라고 부름.
- Node in MRF : joint distribution를 통해 나온 결과를 가짐

아래는 RNN을 뉴런 기반으로 표현한 것이다.    
![RNN](/assets/img/rnn.jpg)

> convolution을 사용해 time steps 에 따라 weight를 공유한 연구들
>  1D temporal(of time) sequence
>  basis for time-delay NN, parameters sharing across time but is shallow

RNN은 2D 정보도 받을 수 있음
 1. spatial data (image)
 2. 시간 관련 데이터 (네트워크에 시간이 거슬러 올라가는 연결이 있을 수 있는...), 전체 시퀀스가 네트워크에 제공되기 전에 관찰됨.

- Classical form of a dynamical system    
    
$$s^{(s)}=f(s^{(t-1)};\theta)$$

