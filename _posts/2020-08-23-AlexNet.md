---
title: AlexNet
comments: true
tags:
- Deep Learning Architecture

toc: true
toc_label: "Helpers"
toc_icon: "cogs"
---

- ## Overview    
    
    ![AlexNet Structure](/assets/img/AlexNet/alexnet_structure.jpg)    

    |CONV1|
    |MAX POOL1|    
    |NORM1|    
    |COVN2|    
    |MAX POOL2|    
    |NORM2|    
    |COVN3|    
    |COVN4|    
    |COVN5|    
    |MAX POOL3|   
    |FC6|    
    |FC7|   
    |FC8|  
     
    - 60M parameteres & 650,000 neurons 가 존재 (왜 이렇게 되는지는 밑에서 자세하게 다룸)     
    - 5 CONV layers ( 몇개는 max-pooling layers를 따름(CONV1, CONV2), 한개라도 없애면 나쁜 성능을 보임)     
    - 3 FC layers    
    - 1000 ways softmax in the end    
    - non-saturating neurons를 사용했다. (ReLUs)     

    
- ## INTRODUCTION    
    
    이 모델은 물체를 더 잘 인식하기 위해 노력하였다. 잘 인식하기 위해서 학습 모델의 성능이 좋아야 하며, 추론을 더 정확하게 해야한다. 그럼 어떻게 해야 학습 모델의 성능이 좋아지고 더욱 잘 물체를 인식할 수 있을까?          
        
    - 학습 모델의 성능을 높히기 위한 조건     
        (추론? 학습? 인지는 안나와있다. 3가지 조건을 모두 만족하면 두 개 모두 성능이 좋아질 것이라 생각한다. 밑에 설명으로 미루어본 내 추측이다.)    
        1. larger datasets => Data augmentation         
        2. learn more powerful models => MLP to CNN    
        3. use better techniques for preventing overfitting => Data augmentation, dropout           
        [excerpt From Here](https://machinelearningmastery.com/improve-deep-learning-performance/)

    - 물체 인식에서 strong & correct assumption이 되기 위한 조건    
        (dataset 크기에 관한)
        1. namely (모델이 정확히 판별해야한다)   
        2. [**stationarity of statistics**]({% link _posts/2020-08-26-stationarity-of-statistics.md %}) (이해가 잘 안됨. 픽셀의 변함이 없어야한다. 어떤게?)   
        3. [**locality of pixel dependencies**]({% link _posts/2020-08-26-stationarity-of-statistics.md %})      
            
        이 세 가지를 [the nature of images]({% link _posts/2020-08-26-stationarity-of-statistics.md %}) 라고 부르고, 이를 만족하면 strong & correct assumption을 할 수 있다고 말할 수 있다.

        - #### stationarity of statistics?
            > 확률 과정 $$X_t: t$$ $$\in$$ $$T$$ 에서 임의의 확률변수 $$X_{t_1},\dotsc,X_{t_k}$$ 에 대한 결합분포 $$F_x(X_{t_1},\dotsc,X_{t_k})$$ 이 있다고 가정하자.    
            > 임의의 $$\tau$$ 에 대해 $$F_x(X_{t_1+\tau},\dotsc,X_{t_k+\tau}) = F_x(X_{t_1},\dotsc,X_{t_k})$$ 를 만족하는 확률 과정 (stochatstic process) 를 의미한다.    

            ```[정리]```    
            ```1. 시간과 상관없이 확률 변수간 확률 분포가 일정하다.```    
            ```2. 분포와 시간이 독립적이라서 확률변수의 기댓값, 분포등 또한 독립이다.```    
            ```3. 이 논문에서는 시간이 아닌 픽셀 관점에서 본 것이라 생각한다. 이미지 관점에서는, Possibly, stationarity of statistics could also indirectly mean that you can use the same filter to detect the same feature in different regions of the image.```    
                
            [\[refer from here\]](https://ai.stackexchange.com/questions/13683/what-is-the-meaning-of-stationarity-of-statistics-and-locality-of-pixel-depen)
        
        - #### locality of pixel dependencies?
            > **neighboring pixels tend to be correlated**, while faraway pixels are usually no correlated. This assumption is usually made in several image processsing techniques (e.g. filters). Of course, the size and the shape of the neightborhood could vary, depending on the region of the image (or whatever), but, in practice, it is usually cohsen to be fixed and rectangular (or squared).    
            
    - CNN의 장점
      - 기존 Feed Forward NN (a.k.a. Multi-Layered Perceptron(MLP)) 보다
        - connections이 적고
        - parameter가 적고
        - easier to train 하다.      

    왜냐하면, MLP는 모든 layer가 Fully Connected 이기 때문에,    
    첫째로, # of total parameters가 레이어를 통과할 때 마다 파라미터의 수가 기하급수적으로 증가한다.     
    둘째로, 공간 정보를 고려하지 않는다. (large-scale datasets이 input으로 들어간다면 메모리용량이 버틸 수 있을까?)     
    [excerpt from Here](https://medium.com/data-science-bootcamp/multilayer-perceptron-mlp-vs-convolutional-neural-network-in-deep-learning-c890f487a8f1)    
    > MLP = 선형 perceptron + multi-layers + non-linear activation     
    > excerpt from [https://en.wikipedia.org/wiki/Multilayer_perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)

    그래서 이 논문에서 CNN을 채택했다. 근데 이론적인 성능은 FFNNs 보다 조금 나쁘다.(not powerful 하다, 성능만 받쳐준다면 언제나 brute force한 방법이 짱인거 같다.)

- ## 특징
    1. Overlapping Maxpooling
    2. ReLUs 처음으로 도입     
    3. GPU를 처음 시도 (with 분산처리) 
    4. Local Response Normalization (LRN) 을 처음 소개함 ( <-> batch normalization ) [괜찮은 참고자료](https://taeguu.tistory.com/29)
    5. overfitting 방지 (이 논문에서는 이걸 많이 신경씀, 이 당시에는 네트워크가 큰 편이였던 것 같음)
        - Dropout -> FC layers 에서 처리, regularization method임 
        - Data Augmentation
        - 하나의 데이터로부터 여러 개를 뽑아냄     
            (Random Crops, Mirror Crops $$\cdots$$)
    6. Dataset은 ImageNet 사용 (15M labeled images, 22,000 categories, 지금은 더 많겠지?) 
    7. 256 X 256 fixed resolution 에서 down sampling 함 => 224 X 224
    8. **NO** pre-processing image ( except for subtarcting the mean activity over the training set from each pixel)
    9. 8번 때문에 이 네트워크에서 (centered) trained RAW RGB values of the pixels
    10. AlexNet Contribution    
    한개의 큰 CNN에서 2D CONV를 GPU로 구현 (잘 optimized 됨)    
    [cuda-convnet2 github link](https://github.com/akrizhevsky/cuda-convnet2)


- ## Outline
    Section 3.Improve its performance & Reduce its training time 할 수 있는 새로운 특징들을 소개     
        
    Section 4. Overfitting 방지를 위한 효과적인 테크닉들     
        
    Section 6. ILSVRC-2012에 쓰인 datasets version을 사용해서 결과를 낸 후 평가    
                (test set labels are unavailable) 1.2M training images, 50,000 validation images, and 150,000 testing images    

- ## Section 3. Architecture

    - ### ReLU Nonlinearity

        - 활성화 함수를 선형 함수로 쓰면 안되는 이유
            > 선형함수인 h(x)=cx를 활성화함수로 사용한 3층 네트워크를 떠올려 보세요. 이를 식으로 나타내면 y(x)=h(h(h(x)))가 됩니다. 이는 실은 y(x)=ax와 똑같은 식입니다. a=c3이라고만 하면 끝이죠. 즉, 은닉층이 없는 네트워크로 표현할 수 있습니다. 뉴럴네트워크에서 층을 쌓는 혜택을 얻고 싶다면 활성화함수로는 반드시 비선형 함수를 사용해야 합니다.    
            Refer from 밑바닥부터 시작하는 딥러닝    
            
        - #### 훈련 시간 관점 with gradient descent 
            - $$tanh(x)$$같은 saturating nonlinearities 는 non-saturating nonlinearity인    
            ($$f(x) = max(0,x)$$) 보다 매우 느리다.       
            ReLU는 이전에 saturating non-linearity를 없애기 위한 노력, $$|tanh(x)|$$, 보다 빠른 학습 속도를 보여준다.    
                
            - 왜 학습 속도가 빠를까?   
                    
                x가 양수이기만 하면 그래디언트는 1로 일정하므로 그래디언트가 죽는 현상을 피할 수 있고, (하지만 무한대 양수로는 갈 수 있다 이것을 피하기 위해 여기서 LRN을 사용하는데, 요즘은 batch normalization을 많이 사용한다.) **미분하기도 편리해 계산복잡성이 낮다.** 실제로 sigmod 또는 tanh 함수 대비 Training error rate가 0.25까지 도달하는 수렴 속도가 6배나 빠르다고 한다.     
                ![ReLUs vs tanh](/assets/img/AlexNet/reluVStanh.jpg)
                [Excerpt From Here](https://ratsgo.github.io/deep%20learning/2017/04/22/NNtricks/)
            
        **ReLUs have the desirable property that they don't require input normalization to prevent them from saturating.**    
        (Gradient Vanishing 걱정 안해도 된다!)  
        (입력의 normalization이 필요 없어짐, 하지만 여러 feature map 에서의 결과를 normalization 해주면 lateral inhibition(측면 억제)이 발생 (generalization 관점에서 훨씬 좋다.))    
        
        - [참고]    
            >      
            > **왜 batch normalization일까?**    
            > We normalize our input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed up learning. If the input layer is benefiting from it, why not do the same thing also for the values in the hidden layers, that are changing all the time, and **get 10 times or more improvement in the training speed.**     
            > - BN은 Activation values를 정규화하는 작업이다. (실제 Data가 Gaussian 되는 것이 아님! batch가 Gaussian이라고 가정만 하는 것)
            > - 입력 분포가 일정해져, learning rate를 크게 설정해도 상관이 없다.    
            > - 학습할때 마다 활성화값/출력값을 정규화하기 때문에 (가중치)초기화 문제에서 비교적 자유로워 진다.   
            > - Deep Network에서 각 weight이 지속적으로 곱해져서 Bad Scaling Effect가 발생한 것을 BN이 bad effect를 상쇄시켜준다.    
            > CONV layer 경우, Activation map 마다 1개의 $$\mu, \sigma^2$$을 구한다.    
            >        
            > 
            > [tanh(x) 특징]    
            > - Squashes numbers to range[-1, 1]    
            > \+ zero-centered: 정규화됨 (엄밀히 말해서 zero-centered는 정규화가 아니다. 하지만 pixel값이 0~255로 fix되어 있기 때문에 정규화라고도 볼 수 있다.)   
            > \- kills gradients when saturated    
            > 
            > [Sigmoid 특징]    
            > - Squashes numbers to range [0, 1]    
            > \- kills gradients when saturated    
            > \- Sigmoid outputs are not zero-centered    
            > \- $$exp()$$ is a bit compute expensive            
            >     
            > - batch normalization + tanh 에 대해 생각해봤다.
            >       BN + ReLUs 조합은 많이 사용한다. 근데 dead ReLU     (will not activate(update), $$\nabla=0$$)가 생기는 경우가 있어,         
                (첫번째 원인으로, Bad Initialization.     
                두번째 원인으로, 너무 높은 학습률(양수로 무한대까지 가서 너무 큰 업데이트 발생-가중치가 점프해버림-데이터의 다양성을 죽임-overfitting))         
            >     
            >   실제로는 10~20%의 Dead ReLU가 발생한다고 한다.(From cs231n 강의) 그래서 ReLU neuron에 positive bias (e.g. 0.01)을 주었는데 효과적이지 못했다. 그래서 나온 것들이 Leaky ReLU, ELU등이 있다.    
            >            
            > [궁금한 점]    
            > 그러면 BN + tanh는 앞서 얘기한 문제를 해결할 수 있지 않을까? 라는 생각을 가지게 되었다.     
            왜냐하면 batch normalization으로 [-1, 1] 사이의 값을 가지게 될 것이고, [-1, 1] 사이의 범위 안에서 tanh는 gradient vanishing에 위배되지 않을 것이므로 output이 정규화까지 되서 batch가 실제로 Gaussian Distribution을 따를 것이라 생각한다.     
            그리고 ReLU의 문제점인 음의 가중치를 갖지 못하는 것 또한 해결이 가능할 것이라 생각하기 때문이다.     
            하지만, 가장 먼저 걸리는 것은 computational cost (1/6배 + $$\alpha$$ where $$\alpha$$=BN cost)이다.    
            혹시 몰라서 BN 논문 링크<https://arxiv.org/pdf/1502.03167.pdf>    

            [해결]            
        ![bn question](/assets/img/AlexNet/BN_question.png)    
               
        - #### Training on Multiple GPUs
            singe GTX 580 GPU: 3GB Memory => limits the max size of the networks that can be trained on it
            그래서, **2개 GPU 사용!**

            내 환경: GTX 1060 6GB 이므로 1개의 GPU 만 사용한다.

          - 2개 GPU가 서로 directly memory access 가 가능해야함 (RW)  **without going through host machine memory**    
              (나중에 꼭 해보고 싶다...)

        - #### parallelization scheme (나는 1개 GPU로 구축해서 적용 못함)
            - puts half of the kernels (or neurons) on each GPU    
            - additional trick : **the GPUs commnunicate only in certain layers**    
            - **2 GPU net takes slightly less time to train than the one-GPU net**    

                - ##### the GPUs commnunicate only in certain layers?
                > e.g.     
                > the kernels of layer 3 take input from all kernel maps in layer 2. However kernels in layer 4 take input only from those kernel maps in layer > 3 which reside on the same GPU.

                - #### 2 GPU net takes slightly less time to train than the one-GPU net?
                > the final convolutional layer에서 (one-GPU 의 커널 수 == two-GPU 의 커널 수)    
                >         
                > 그 이유는 net's parameters 의 대부분이 (마지막 CONV layer로 부터 input을 받는) 첫번째 FC layer 에 있기 때문이다.      
                > 그래서, 두개의 네트워크에서 대략적으로 같은 수의 파라미터를 가지도록하기 위해서 마지막 CONV layer의 사이즈를 반으로 줄이지 않았다. 
                >    
                > 그래서, 이 비교는 one-GPU net 쪽으로 편향되어 있다, since it is bigger than "half the size" of the two-GPU net.

    - ### Local Response Normalization (LRN)
        - ReLUs가 input normalization이 필요 없지만 은 generalization을 도와준다.    
        - activation function으로 ReLU를 사용했을 때, 장점은 sigmoid나 tanh와 달리 결과값이 양수에 한해서는 막히지 않았기 때문에 (no gradient vanishing)
        입력하는 데이터에 대해서 normalization이 필요하지 않았다. 그럼에도 양수 방향으로 무한히 커질 가능성이 있어 너무 큰 값이 주변 값들을 무시할 수 있기 때문에 normalization(local normalization scheme)을 해주는게 좋다.     
        그래서 일반적인 관점(generalization)을 도와주는 local normalization scheme 을 찾음. 그게 LRN 이다. (요즘에는 LRN 잘 사용 안하고 batch normalization 사용, 이유 찾아보기!)     

        - applied this normalization after applying the ReLU nonlinearity in certain layers (Section 3.5)     

        - $$a^{i}_{x,y}$$ : the activity of a neuron computed by applying ***kernel $$i$$*** at ***position $$x,y$$***    
        Then, applying the ReLU nonlinearity,    
        the ***response-normalizaed activity $$b^{i}_{x,y}$$*** is given by the expression    
        $$ b^{i}_{x,y} = a^{i}_{x,y} / \bigg(k + \alpha \displaystyle\sum_{j=max(0,i-n/2)}^{min(N-1, i+n/2)} (a^{j}_{x,y})^2 \biggr)^{\beta} $$    
        where the sum runs over ***$$n$$ "adjacent" kernel maps*** at the same spatial position, and $$N$$ ***is the total # of kernels in the layer***    
        $$k, n, \alpha, and$$ $$\beta$$ : hyper-parameters whose values are determined using a validation set.    
        (in this paper, using $$k = 2, n = 5, \alpha = 10^{-4}, and$$ $$\beta = 0.75$$)     
        ![LRN](/assets/img/AlexNet/LRN.png)

        - The ordering of the kernel(filter) maps : arbitrary and determined before training begins    
        - modeling like lateral inhibition : 하나의 뉴런이 다른 뉴런들을 억제하는 경향을 따라함    
        - 여기서 나온 scheme은 local contrast normalization scheme [\[11\]](https://ieeexplore.ieee.org/document/5459469)과 유사   
        - 요즘은 이거 안쓰고 batch normalization 사용
          - [왜 LRN 안쓰고 BN 사용할까?]({% link _posts/2020-08-31-Local-Response-Normalization.md %})

    - ### overlapping maxpooling
        overlapping pooling은 덜 overfitting 한다.     
        AlexNet에서 3X3 크기의 커널, stride=2로 설정하여 풀링이 겹치게 하였다.
        이 결과, top-1 error는 0.3, top-5 error는 0.4 정도 성능 개선이 있었다.

    - ### Overall Architecture
        Our network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution.        
        트레이닝 케이스들의 확률 분포 아래 정확한 label의 (확률의 log)의 평균을 최대화 합니다!

        - 2,4,5 CONV layers 는 오직 그 이전의 layer가 있던 같은 GPU 안의 그들의 kernel maps으로 연결됨    
        - 3번째 CONV layer는 2번째 CONV layer 안에 있는 모든 kernel maps과 연결됨    
        - Response-normalization layers는 1번째 layer와 2번째 layer를 따름
        - Max-pooling layers는 reponse-normalization layers 와 5번째 CONV layer를 같이 따름
        - ReLU non-linearity 는 모든 CONV & FC layers의 output에 적용됨

        ![AlexNet Structure](/assets/img/AlexNet/alexnet_structure.jpg)
        - 1번째 CONV layer는 224 X 224 X 3 input image를 (커널 사이즈가 11 X 11 X 3 사이즈 그리고 stride가 4 pixels 인) 96개의 커널
            FC layer의 뉴런들은 이전 layer의 모든 neurons들과 연결됨


        - 총 60M parameters가 있음
- ## Section 4. Reducing Overfitting

    이 연구는 overfitting을 고려하지 않으면 learning 하기 insufficient하다. 

    - ### 1. Data Augmentation
        > overfitting을 감소시키는 가장 쉽고 common method는 내가 데이터셋을 만들어 늘리자! (인공적으로)     
        > transformation img = image translations * horizontal reflections    
        > (10 = 5 * 2)

        - #### 1-1. First Form

            - image translations =     
                - center patch    

                | | | |
                | |1| |
                | | | |    

                - top left corner patch    

                |1| | |
                | | | |
                | | | |
                    

                - top right corner patch     

                | | |1|
                | | | |
                | | | |    

                - bottom left corner patch     

                | | | |
                | | | |
                |1| | |    

                - bottom right corner patch     

                | | | |
                | | | |
                | | |1|    


            - #### 수평 reflections (좌우반전)=     

                | |\|| |
                |8|\||8|
                | |\|| |

            - transformation image는 label-preserving 이고 disk에 저장할 필요 없다. 그래서 computationally free 하다
            - transformation image는 CPU에서 python 코드로 제작되고, GPU에서는 previous batch 를 training한다.     
            - 이 과정에서 256X256 image를 224X224로 pre-processing 한다 (extracted patches) (이거 말고 pre-processing과정 없음, input이 224X224X3 인 이유)
            - 1개의 이미지로 부터 2048개의 이미지를 만들어낼 수 있었다. (10*204.8? 어떻게?)
            - 10 patches에 대해, network의 softmax layer에서 만들어진 predictions를 averaging 한다.    

            > - 궁금한 점    
            > 왜 8 corners, vertical 를 안쓸까? 그럼 총 9개 transformation images * 2 horizental * 2 vertical = 36배로 늘어나서 더 training이 > > > 잘되지 않을까?
            > training에 근접해서 오히려 overfitting 유발할까 ?

        - #### 1-2. Second Form
            > altering the intensities of the RGB channels in training images.

            in details,    
            the set of RGB pixel values 에다가 ImageNet training set을 통해 PCA를 perform 한다.    
                
            each training image,      
            we add multiples of the 찾아낸 principal components ($$I_{xy}$$의 각 채널을 말하는듯), with magnitudes proportional to the corresponding ( eigenvalues X a random variable ) from a $$N(0,0.1)$$     
                

            - 따라서, each RGB image pixel, $$I_{xy} = [I^R_{xy},I^G_{xy},I^B_{xy}]^T$$ 에다가 다음 quantity를 더한다.    
            

                $$[p_1,p_2,p_3][\alpha_1\lambda_1,\alpha_2\lambda_2,\alpha_3\lambda_3]^T$$    
                where    
                $$p_i = eigenvector$$    
                $$\lambda_i = eigenvalue$$    
                $$\alpha_i$$ = a random variable
                of the 3 X 3 covariance matrix of RGB pixel values    
                
            $$\alpha$$는 특정 training image가 다시 training에 사용되기까지 그 image의 모든 pixel에 대해서 오직 1번만 drawing 된다.    
            image가 다시 사용될때, 그 point에서 re-drawn 된다.

            - This scheme approximately captures **an important property** of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination.
            - This scheme reduces the top-1 error rate by over 1%

    - ### 2. Dropout

        - **Combining the predictions** of many different models is a very successful way to <U>reduce test errors.</U>
            > - 근데, too expensive for big NN > efficient version of model combination은 training 동안 오직 cost가 2배씩 상승한다. (아직도 많은거 같은데?)

        - Prob = 0.5 인 each hidden neuron의 output을 0으로 "dropped out" 한다.
        - "dropped out" 된 neurons 은 feed forward에 don't contribute & back propagation 에 don't participate
        - So, dropout 방식은 complex co-adaptations of neurons를 줄인다. (왜냐하면 뉴런은 특정 다른 뉴런들의 존재에 의존하지 못하기 때문이다.)
            - 궁금한 점    
            뉴런이 particular other neurons의 presence에 의존하지 못한다... 그럼 LRN은?? 어느 뉴런이 강하게 반응할때 주변 뉴런을 억제하는 경향이다. 이 말은 한 뉴런이 다른 뉴런에 대해 의존적이다... ? (rely on과 depends on의 뉘앙스 차이..........)

        - 그러므로, 좀 더 robust features를 배워야 하는게 강제된다. 여기서 robust features는 다른 뉴런들의 many different random subsets과 함께 유용하다.
            > - 아리까리 한거: 왜 남아있는 것들이 robust 한지?
            many different random subsets... 은 dropout이 끝나고 남은 다른 뉴런들의 parameter 인가?    
            
        - test time, we use all the neurons but (their outputs X 0.5)     
        0.5: which is the geometric mean of the 지수적으로 많은 (급속도로 많아지는) dropout networks로 부터 만들어진 예측 분포로 부터 taking 한 a reasonable approximate    
            - 큰수의 법칙 때문인듯?    
            - test time때 output을 절반으로 줄여야 한다. (training time때 절반의 뉴런만 학습되었기 때문에 맞춰줘야함. 아직 와닿지는 않음)

        - 2개의 FC layers에 dropout 사용. Dropout은 수렴하기 위해 iterations 갯수의 두배정도 필요하다.
            

    - ## Section 5. Details of learning

        - SGD + momentum 사용
        - batch size : 128 examples
        - momentum : 0.9
        - weight decay : 0.0005 (not merely a regularizer, it reduces the model's training error)
            - The update rule for weight $$w$$ was    
                
                $$v_{i+1} := 0.9 \cdot v_i - 0.0005 \cdot \epsilon \cdot w_i - \epsilon \cdot \langle \frac {\partial L}  {\partial w} | _{w_i} \rangle_{D_i}$$     
                $$w_{i+1} := w_i + v_{i+1}$$    

                where     
                $$i$$ : the iteration index    
                $$v$$ : the momentum variable    
                $$\epsilon$$ : the learning rate     
                $$\langle \frac {\partial L}  {\partial w} | _{w_i} \rangle_{D_i}$$ : the average over the $$i$$th batch $$D_i$$ of the derivative of the objective with respect to $$w$$, evaluted at $$w_i$$     
        - weights 초기화 : N(0, 0.01) (in each layer)
        - neuron biases 초기화
            - 2nd, 4th, 5th CONV layers, and FC hidden layers : constant 1
            - 나머지 layers : constant 0
            - 초기화는 ReLU에서 나온 positivie inputs을 학습하는 the early stages 를 accelerates 한다.
            - 각 layer의 DoF는 1이라서 (일변수이기 때문에??)    
                무조건 bias 1을 더해주는지 알았다. 그냥 공간상의 1을 더해주는 건지, 실제로 bias 가 있다는 의미의 1인지...?
                왜 bias 0 이 성립이 되는지 확인

        - learning rate 초기화 : 0.01
            - 모든 layers의 learning rate는 동일하게 한다. (training을 통해 조정해나감)
            - heuristic : learning rate / 10 ( 현재 learning rate에서 validation error rate가 증가하는 것을 멈췄을 때 나눠줌)
            - **termination 전에** 3배 감소한다.

        - training set : 1.2M images
        - trained cycles : 90
        - training period : 5~6 days on two NVIDIA GTX 580 3GB GPUs

    - ## 6. Result

        - ILSVRC-2010 summary     

            |Model|Top-1|Top-5|
            |Sparse coding|47.1%|28.2%|
            |SIFT + FVs|45.7%|25.7%|
            |CNN|37.5%|17.0%|    

            - 논문에서 안쓰였는데 ImageNet 2009 fall version에서 사용한 리포트에서 비교할때 쓰인 것
                - SIFT: distribution of features    
                - Fisher Vector: encodes the gradients of the log-likelihood of the features under the Gaussian Mixture Model, with respect to the GMM parameters

        - Qualitative Evaluations

        <U>the network has learned a variety of frequency- and orientation-selective kernels, as well as various colored blobs</U>
        무슨말인지 모르겠음 나중에 확인

        - GPU1
            - specialized in largely color-agnostic (색깔 인지불능)
        - GPU2
            - specialized in largely color-specific

        2개의 specialization은 돌아갈 때 마다 any particular random weight initialization 에 독립적이다. 

        두 이미지가 마지막 4096 hidden layer를 지나 생성된 feature activations vectors 가 얼마 차이 안날때 (Euclidean seperation between two 4096-dimensional, real-valued vectors), 두 이미지는 유사하다고 볼 수 있다.    
        하지만, **auto-encoder를 훈련시키면 to compresss these vectors to short binary codes 더 효율적으로 만들 수 있다.**    
            
        this should produce a much better image retrieval method than applying auto-encoders to the raw pixels, which does not make use of image labels and hence has a tendency to retrieve images with similar patterns of edges, whether or not they are semantically similar. 

        at the pixel level, the retrieved training images are generally not close in L2 to the query images in the first column.
        => 만약에 여러 포즈를 취하는 개 사진이 여러 개가 있다면 결과를 내기 위해서는 더 많은 test images가 보충되어야 함.



> [참고]     
     Choosing the pattern of connectivity is a problem for cross-validation    
        - 훈련해야할 파라미터 수    
           (weight + bias) * # of feature maps ( bias 는 보통 1)
