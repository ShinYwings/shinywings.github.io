---
title: "[ECCV 2020] NeRFs: Neural Radience Fields for View Synthesis"

categories:
- 논문 리뷰
tags:
- NeRF
  
comments: true
toc: true

---
> 이 포스트는 [pulluper 님의 블로그](https://csm-kr.tistory.com/64) 내용을 기반으로 재구성하였습니다.

## 1. Introduction
![](/assets/img/nerf/Picture1.jpg)  

특정 물체를 다양한 각도에서 촬영한 여러 Scene을 기반으로, 입력에 없던 **카메라 시점**의 새로운 Scene을 생성하는 방법을 연구한 논문입니다.  

![](/assets/img/nerf/Picture2.jpg)   

여러 이미지에서 동일한 물체를 촬영했다고 가정해봅시다. 각 이미지의 촬영 위치 $o=\{x',y',z'\}$와 촬영 방향 $d=\{\theta, \phi\}$를 알고 있다면,  
촬영 위치 $o$에서 촬영 방향 $d$로 Ray를 그릴 수 있습니다. 이때, 서로 다른 이미지에서 나온 Ray들이 교차하는 지점을 통해  
물체의 위치 $p=\{x,y,z\}$, 색상 $c=\{r,g,b\}$, 그리고 밀도 $\sigma$를 추정할 수 있습니다.  
따라서, 이 논문의 목표는 $(x,y,z,\theta,\phi)$로부터 $(r,g,b,\sigma)$를 추출하는 모델 $F_{\Theta}$를 구하는 것이며, 이는 다음과 같은 수식으로 표현됩니다.  

$$F_{\Theta}: (x,y,z,\theta,\phi) \rightarrow (r,g,b,\sigma)$$   

## 2. Methodology
> 이 포스트는 추론 과정과 실험 결과를 제외하고, 아이디어 및 훈련 과정에 초점을 맞춰 작성되었습니다.  

아래 그림은 NeRFs $F_{\Theta}$의 전체 구조를 보여줍니다. 모델 훈련은 총 4(+1)단계로 구성됩니다.  
![](/assets/img/nerf/Picture3.jpg)   
  
프로세스는 `(a)Sampling`$\rightarrow$`(+)Positional Encoding`$\rightarrow$`(b)Radiance Fields Estimation`$\rightarrow$`(c)Volume Rendering`$\rightarrow$`(d)Loss` 순서로 진행됩니다. 각 단계를 자세히 살펴보겠습니다.  

### (a) Sampling  
아래 그림과 같이, `(1)각 2D 이미지로부터 Ray 생성`하고 `(2)각 Ray를 여러 포인트로 샘플링`하여 각 포인트의 위치와 촬영 방향 $(x,y,z,\theta,\phi)$를 출력하는 과정입니다.    
![](/assets/img/nerf/Picture4.jpg)*출처: https://csm-kr.tistory.com/64*   

#### **2.(a).1. 각 2D 이미지로부터 Ray를 생성**

Ray는 다음과 같은 직선의 방정식으로 정의됩니다.  

$$ R(p) = o+p*d $$   

여기서 $o$는 시작 위치, $d$는 방향 벡터, $p$는 Ray를 구성하는 샘플 포인트입니다.

Ray를 생성하려면 2D 이미지와 해당 이미지의 Pose(=Rotation+Translation) 정보가 필요합니다. 이때, Pose의 Rotation은 $d$, Translation은 $o$에 사용됩니다.  

> **Pose란?**  
> Pose(a.k.a. Camera extrinsic parameter)와 이후 설명될 Camera intrinsic parameter에 대한 자세한 내용은 [이 블로그](https://csm-kr.tistory.com/64)를, Ray 생성 과정에 대한 자세한 설명은 [이 블로그](https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-generating-camera-rays/generating-camera-rays.html)를 참조하세요.   

> **Input 값은 어떻게 얻을까?**  
> 일반적인 이미지들은 [Camera Calibration](https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html)을 통해 Undistortion Image로 변환한 후 Input으로 사용해야 합니다. Pose 정보는 OpenCV의 SolvePnP 함수나 SfM 도구(e.g., COLMAP)를 사용해 추출할 수 있습니다.  

Input의 Pose는 "$물체 \rightarrow 이미지$" 방향의 Ray 정보를 제공합니다. 하지만 우리가 원하는 $o$와 $d$는 "$이미지 \rightarrow 물체$" 즉, Pose와 반대 방향의 Ray 정보를 필요로 하며, 이를 얻기 위해 아래 수식을 사용합니다.  
$$
\begin{equation}
픽셀 좌표계(x,y,1) = K * Pose_{world2cam} * 월드좌표계(X_w, Y_w, Z_w)
\end{equation}
$$  

위 수식을 "$이미지 \rightarrow 물체$" 방향으로 역변환하면 다음과 같습니다.  
$$
\begin{equation}
월드좌표계(X_w, Y_w, Z_w) = Pose_{cam2world} * K^{-1} * 픽셀 좌표계(x,y,1)
\end{equation}
$$  

여기서 $K$는 Camera Intrinsic Parameter입니다.  

수식 (2)를 도출하기 위해, 수식 (1)을 세부적으로 분석해 보겠습니다.  

- **World 좌표계에 정의된 3D 포인트 $\{X_w, Y_w, Z_w\}$를 Camera 좌표계 $\{X_c, Y_c, Z_c\}$로 변환해줍니다.**   
  
$$
    \begin{bmatrix}
        X_c \\
        Y_c \\
        Z_c
    \end{bmatrix}
    =
    Pose_{world2cam}
    \begin{bmatrix}
        X_w \\
        Y_w \\
        Z_w
    \end{bmatrix}
$$   

- **Camera 좌표계에 정의된 3D 포인트 $\{X_c, Y_c, Z_c\}$를 동차좌표계인 Normalized 좌표계로 변환해줍니다.**   
  
$$
    \begin{bmatrix}
        u \\
        v \\
        1
    \end{bmatrix}
    =
    \begin{bmatrix}
        X_c/Z_c \\
        Y_c/Z_c \\
        1
    \end{bmatrix}
$$  

- **Normalized 좌표계로부터 카메라 행렬 $K$ (Intrinsic parameter)를 통해 최종 이미지 좌표계를 만들어줍니다.**   
  
$$
    \begin{bmatrix}
        x \\
        y \\
        1
    \end{bmatrix}
    =
    \begin{bmatrix}
    f_x & 0 & c_x \\
    0 & f_y & c_y \\
    0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
    u \\
    v \\
    1
    \end{bmatrix}
$$

앞서 세부적으로 분석한 수식들을 역으로 계산하여 수식 (2)를 도출합니다.
- **이미지 좌표계 to Normalized 좌표계**   
  
$$
    \begin{bmatrix}
        u \\
        v \\
        1
    \end{bmatrix}
    =
    \begin{bmatrix}
    1/f_x & 0 & 0 \\
    0 & 1/f_y & 0 \\
    0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
    1 & 0 & -c_x \\
    0 & 1 & -c_y \\
    0 & 0 & 1
    \end{bmatrix}
    
    \begin{bmatrix}
    x \\
    y \\
    1
    \end{bmatrix}
$$

- **Normalized 좌표계에 정의된 Pixel** $\{u, v, 1\}$ **의 Z축은 광축입니다. 이미지로 들어오는 빛이 아닌 이미지로부터 나가는 빛을 모사해야하므로, Z축을 반대로 해줍니다. 더불어, 프로그래밍할 때 배열 순서와 카메라 좌표계의 y축 방향은 서로 반대입니다. 이를 반영해줍니다.**   
  
$$
    \begin{bmatrix}
        u \\
        -v \\
        -1
    \end{bmatrix}
    =
    \begin{bmatrix}
        1 && 0 && 0 \\
        0 && -1 && 0 \\
        0 && 0 && -1 \\
    \end{bmatrix}
    \begin{bmatrix}
        u \\
        v \\
        1
    \end{bmatrix}
$$  

- **Normalized 좌표계에 정의된 Pixel** $\{u, -v, -1\}$ **을 Pose의 역함수를 통해 World 좌표계** $\{X_w, Y_w, Z_w\}$ **로 변환할 수 있습니다.**   
  
$$
    \begin{bmatrix}
        X_w \\
        Y_w \\
        Z_w
    \end{bmatrix}
    =
    Pose_{cam2world}
    \begin{bmatrix}
        u \\
        -v \\
        -1
    \end{bmatrix}
$$  

$o$, $d$ 는 위 수식으로부터 얻을 수 있습니다.   

$$
[R|T] = Pose_{cam2world}
$$    

$$ 
  o = T
$$

$$
  d = \left\| R * 
  \begin{bmatrix}
        u \\
        -v \\
        -1
    \end{bmatrix} \right\|
$$   

여기서 d는 방향 벡터이므로, normalization을 해줍니다.
#### **2.(a).2. 각 Ray를 여러 포인트로 샘플링**  
다음은 p가 하이퍼파라미터로, 저자는 64와 128의 값을 사용했습니다. 예를 들어, 0부터 63까지 일정 간격으로 넘버링을 적용한 경우를 들 수 있습니다.  
  
이를 통해 위에서 계산된 모든 값을 $R(p) = o + p \cdot d$에 대입하여, 아래 그림과 같은 샘플 포인트들을 단일 이미지로부터 추출할 수 있습니다. 각 포인트의 $(x, y, z)$ 좌표와 뷰 방향 $(\theta, \phi)$를 생성하여 NeRF 모델의 입력으로 활용합니다.

> 참고로, NeRF는 일반적으로 100개 이상의 이미지를 사용합니다. 아래 그림에서 확인할 수 있듯이, 연산량이 매우 많아 A100 GPU로도 1~2일이 소요됩니다. 이러한 문제를 해결하기 위한 후속 연구들이 다수 발표되고 있습니다.  
  
![](/assets/img/nerf/Picture4.jpg)*출처: https://csm-kr.tistory.com/64*   

### (+) Positional Encoding (PE)  
샘플된 포인트에 Positional Encoding을 적용하는 과정입니다.  

NeRF 모델의 Input으로 샘플된 포인트를 직접 사용하는 대신, Positional Encoding을 적용한 Feature Vector를 사용하면 이미지 내 고주파 성분을 더 잘 표현할 수 있어 디테일이 향상됩니다. (*출처: https://csm-kr.tistory.com/64*)  

| Positional Encoding 적용 전        | Positional Encoding 적용 후        |
| ---------------------------------- | ---------------------------------- |
| ![](/assets/img/nerf/Picture6.gif) | ![](/assets/img/nerf/Picture8.gif) |
| ![](/assets/img/nerf/Picture7.gif) | ![](/assets/img/nerf/Picture9.gif) |

각 샘플 포인트의 위치 $P = (x,y,z)$와 View direction $D = (\theta, \phi)$, 총 5개의 파라미터에 Fourier Positional Encoding을 적용하여 각각 Feature Vector를 생성합니다.  
Fourier Positional Encoding에 대한 자세한 내용은 [[개념 정리] Positional Encoding](/posts/PositionalEncoding/index.html)을 참조하세요.  

$$\gamma_{𝐿} (u_i)=(\sin⁡(2^{0} u_i ),\cos(2^{0} u_i ),\cdots,\sin⁡(2^{𝐿−1} u_i ),\cos (2^{𝐿−1} u_i ))$$    

![](/assets/img/nerf/Picture5.jpg)  

> 논문에서는 $\sin⁡(2^{𝐿−1} \pi u_i )$로 표현되었지만, 코드에서는 $\pi$가 제거되었습니다. 이는 훈련에 사용된 데이터가 [-1, 1] 범위의 위치 값을 가질 것으로 예상했으나 실제로는 [-1.5, 1.5] 범위를 가졌기 때문입니다. ([참고: 저자 Q&A](https://github.com/bmild/nerf/issues/12))  
> 중요한 점은 Positional Encoding을 설정할 때 sin, cos 값이 한 주기를 초과하지 않도록 해야 한다는 것입니다.  

### (b) Radiance Fields Estimation   
학습 모델을 통해 포인트의 색상과 밀도 $(r,g,b,\sigma)$를 추정하는 과정입니다. 간단한 MLP를 사용하므로 자세한 설명은 생략합니다.  
![](/assets/img/nerf/Picture10.jpg)   
- 초록색 박스 $\gamma(x)$는 Positional Encoding으로부터 생성된 Input 값이며, 60은 한 Ray에서 샘플링된 포인트의 개수를 나타냅니다.  
- 파란색 박스는 256차원의 Fully Connected Layer를 의미합니다.   
- 빨간색 박스는 모델의 Output을 나타냅니다.  

### (c) Volume Rendering  
Ray가 통과하는 각 샘플 포인트로부터 누적 계산을 수행하여 새로운 Scene을 생성하는 과정입니다.  

$$\sum^{N}_{i=1} T_{i}*(1-\exp(-\sigma_{i}\delta_{i}))*c_i$$    
$$T_{i} = \exp(-\sum^{i-1}_{j=0} \sigma_{j}\delta_{j})$$  

각 항의 의미는 다음과 같습니다.  
- $T_i$: 누적 투과율 (Accumulated Transmittance)  
- $(1-\exp(-\sigma_{i}\delta_{i}))$: 볼륨 밀도 (Volume Density)  
- $c_i$: 색상 (Radiance) $\equiv(r,g,b)$  

여기서 $\delta$는 샘플 포인트 간의 간격을 의미합니다.  
아래는 Accumulated Transmittance와 Volume Density의 경향성을 보여줍니다.  

| Accumulated Transmittance           | Volume Density                      |
| ----------------------------------- | ----------------------------------- |
| ![](/assets/img/nerf/Picture12.jpg) | ![](/assets/img/nerf/Picture13.jpg) |

- Accumulated Transmittance는 $x=0$일 때 $y=1$이며, $x$가 증가함에 따라 $y$가 0에 수렴하는 단조 감소 함수입니다.  
- 반대로, Volume Density는 $x=0$일 때 $y=0$이며, $x$가 증가함에 따라 $y$가 1에 수렴하는 단조 증가 함수입니다.  

수식만으로 이해하기 어려운 경우, 아래 예시를 통해 Volume Rendering을 살펴볼 수 있습니다. (참조: [xoft 블로그](https://xoft.tistory.com/))  

$N=5$, $\delta_i=1$인 Ray를 따라 샘플 포인트 $u_{i}=\{c_i, \sigma_i\}$가 아래 그림과 같이 배치되어 있다고 가정합니다.  

![](/assets/img/nerf/Picture11.jpg)

- $i=1,2$: $T_i$가 1이고 Volume Density가 0이므로 전체 값이 0이 됩니다.  
- $i=3$: $T_i$는 0~1 사이의 값을 가지며, Volume Density는 1이 됩니다.  
- $i=4,5$: $T_i$는 0에 수렴하며, Volume Density는 0~1 사이의 값을 가집니다.  

이를 통해 $T_i$가 클수록 빛이 해당 지점을 통과했음을, 작아질수록 물체에 부딪혔음을 알 수 있습니다.  
또한, Volume Density는 해당 지점에서 물체의 밀도와 특성을 나타냅니다.  

### (d) Loss  
새로운 Scene에서 생성된 2D 픽셀 색상이 실제 Ground Truth와 얼마나 일치하는지 평가하는 과정입니다.  
Loss 함수는 Coarse-grained 모델의 Output $\hat{C}_c(r)$와 Fine-grained 모델의 Output $\hat{C}_f(r)$ 각각을 Ground Truth $C(r)$와 비교하여 L2 Loss를 계산한 후 선형 결합합니다.  

$$L(r) = \|\hat{C}_c(r) - C(r) \|^2_2 + \|\hat{C}_f(r) - C(r) \|^2_2$$

Coarse-grained 모델과 Fine-grained 모델의 차이는 샘플 포인트 추출 방식에 있습니다.  
- **Coarse-grained 모델**: Ray를 균일한 간격으로 샘플링하여 포인트를 추출합니다.  
- **Fine-grained 모델**: Volume Rendering으로 계산된 각 포인트의 가중치  
  $$w_i = T_{i}*(1-\exp(-\sigma_{i}\delta_{i}))$$  
  를 기반으로 물체가 있을 가능성이 높은 영역을 중심으로 Importance Sampling을 수행합니다.  

## 3. Closing  
최종 목표는 Gaussian Splatting (GS) 연구이며, GS의 관련 연구를 조사하기 위해 NeRFs를 분석했습니다.  
다음 포스트에서는 NeRFs를 개선한 후속 논문인 MipNeRFs, Instant-NGP, PlenOctree를 다룰 예정입니다.
