---
title: "[ECCV 2020] NeRFs: Neural Radience Fields for View Synthesis"

categories:
- 논문 리뷰
tags:
- NeRF
  
comments: true
toc: true

---
> 이 포스트는 [pulluper 님의 블로그](https://csm-kr.tistory.com/64) 내용을 기반으로 재구성하였습니다.

## 1. Introduction
![](/assets/img/nerf/Picture1.jpg)  

특정 물체에 대해 다양한 각도에서 여러 Scene을 촬영하여, 그 안에 입력 에 없던 **카메라 시점**의 새로운 Scene을 만들어내는 방법에 관한 연구입니다.  

![](/assets/img/nerf/Picture2.jpg)   

같은 물체를 바라보는 서로 다른 두 이미지가 있다고 가정해봅시다. 각 이미지에 나타난 점이 서로 같다는 것을 알고 있다면, 그 점으로부터 촬영 방향 $d=\{\theta, \phi\}$를 향해 직선 Ray를 그릴 수 있습니다. 이때, 서로 다른 이미지에서 나온 각 직선 Ray들이 만나는 지점 $p=\{x,y,z\}$에 어떤 3D 포인트가 있을 것이고 그 포인트의 색상 $c=\{r,g,b\}$과 밀도 $\sigma$를 구할 수 있습니다.   
따라서, 이 논문의 목표는 $(x,y,z,\theta,\phi)$로부터 $(r,g,b,\sigma)$를 추출하는 모델 $F_{\Theta}$ 를 구하는 것이고, 다음과 같은 수식으로 나타낼 수 있습니다.  

$$F_{\Theta}: (x,y,z,\theta,\phi) \rightarrow (r,g,b,\sigma)$$   

## 2. Methodology
> 이 포스트는 추론 과정과 실험 결과는 제외한 Idea 및 훈련 과정에 대한 조사만 진행했습니다.  
> 
아래 그림은 NeRFs $F_{\Theta}$를 보여줍니다. 모델 훈련을 위해 총 4(+1)단계의 프로세스가 진행됩니다.  
![](/assets/img/nerf/Picture3.jpg)   
  
프로세스 순서는 `(a)Sampling`$\rightarrow$`(+)Positional Encoding`$\rightarrow$`(b)Radiance Fields Estimation`$\rightarrow$`(c)Volume Rendering`$\rightarrow$`(d)Loss`
와 같습니다. 그러면 각각에 대해 자세히 살펴보겠습니다.  

### (a) Sampling  
아래 그림과 같이, `(1)각 2D 이미지로부터 Ray 생성`하고 `(2)각 Ray를 여러 포인트로 샘플링`하여 각 포인트의 위치 그리고 촬영 방향 $(x,y,z,\theta,\phi)$를 출력하는 과정입니다.    
![](/assets/img/nerf/Picture4.jpg)*출처: https://csm-kr.tistory.com/64*   

#### **2.(a).1. 각 2D 이미지로부터 Ray를 생성**

먼저, Ray는 다음과 같이 직선의 방정식으로 정의됩니다.  

$$ R(p) = o+p*d $$   

$o$는 시작 위치, $d$는 방향 벡터, 그리고 $p$는 Ray를 구성하는 샘플 포인트 입니다.

먼저, Ray를 생성하기 위해 2D 이미지와 그 이미지의 Pose(=Rotation+Translation)정보가 입력으로 필요합니다. 여기서, Pose의 Rotation은 $d$ 그리고 Translation 값은 $o$ 에 사용됩니다.
> **Pose?**  
> Pose(a.k.a. Camera extrinsic parameter) 그리고 이 후 설명될 Camera intrinsic parameter에 관련된 설명은 [이 블로그](https://csm-kr.tistory.com/64)를, 그리고 Ray 생성 과정에 대한 자세한 설명은 [이 블로그](https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-generating-camera-rays/generating-camera-rays.html)를 참조해주세요.   

> **Input 값은 어떻게 얻을까?**  
> 스마트폰등을 사용하여 얻은 일반적인 이미지들은 [Camera Calibration](https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html)을 사용해서 얻은 Undistortion Image를 그 이미지 대신 Input으로 사용해야 합니다. Pose 정보는 OpenCV의 SolvePnP함수 또는 SfM 도구(e.g., COLMAP)를 사용해서 가져와야합니다.   

Input의 Pose는 "$물체 \rightarrow 이미지$" 를 향하는 Ray 정보를 가지고 있습니다. 하지만 우리가 원하는 $o$와 $d$ 는 그 반대로 향하는 Ray 정보를 필요로 하고, 두 값을 얻기 위해 아래 수식을 이용합니다.  
$$
\begin{equation}
픽셀 좌표계(x,y,1) = K * Pose_{world2cam} * 월드좌표계(X_w, Y_w, Z_w)
\end{equation}
$$  

해당 수식을 다음과 같이 "$이미지 \rightarrow 물체$" 로 역변환 해줍니다.  
$$
\begin{equation}
월드좌표계(X_w, Y_w, Z_w) = Pose_{cam2world} * K^{-1} * 픽셀 좌표계(x,y,1)
\end{equation}
$$  

여기서 $K$는 Camera Intrinsic Parameter입니다.  

수식 (2)를 만들기 위해, 수식 (1)을 break down 해보겠습니다.  

- **World 좌표계에 정의된 3D 포인트 $\{X_w, Y_w, Z_w\}$를 Camera 좌표계 $\{X_c, Y_c, Z_c\}$로 변환해줍니다.**   
  
$$
    \begin{bmatrix}
        X_c \\
        Y_c \\
        Z_c
    \end{bmatrix}
    =
    Pose_{world2cam}
    \begin{bmatrix}
        X_w \\
        Y_w \\
        Z_w
    \end{bmatrix}
$$   

- **Camera 좌표계에 정의된 3D 포인트 $\{X_c, Y_c, Z_c\}$를 동차좌표계인 Normalized 좌표계로 변환해줍니다.**   
  
$$
    \begin{bmatrix}
        u \\
        v \\
        1
    \end{bmatrix}
    =
    \begin{bmatrix}
        X_c/Z_c \\
        Y_c/Z_c \\
        1
    \end{bmatrix}
$$  

- **Normalized 좌표계로부터 카메라 행렬 $K$ (Intrinsic parameter)를 통해 최종 이미지 좌표계를 만들어줍니다.**   
  
$$
    \begin{bmatrix}
        x \\
        y \\
        1
    \end{bmatrix}
    =
    \begin{bmatrix}
    f_x & 0 & c_x \\
    0 & f_y & c_y \\
    0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
    u \\
    v \\
    1
    \end{bmatrix}
$$

앞서 break down한 수식들을 거꾸로 연산하여 수식 (2)를 만듭니다.   
- **이미지 좌표계 to Normalized 좌표계**   
  
$$
    \begin{bmatrix}
        u \\
        v \\
        1
    \end{bmatrix}
    =
    \begin{bmatrix}
    1/f_x & 0 & 0 \\
    0 & 1/f_y & 0 \\
    0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
    1 & 0 & -c_x \\
    0 & 1 & -c_y \\
    0 & 0 & 1
    \end{bmatrix}
    
    \begin{bmatrix}
    x \\
    y \\
    1
    \end{bmatrix}
$$

- **Normalized 좌표계에 정의된 Pixel** $\{u, v, 1\}$ **의 Z축은 광축입니다. 이미지로 들어오는 빛이 아닌 이미지로부터 나가는 빛을 모사해야하므로, Z축을 반대로 해줍니다. 더불어, 프로그래밍할 때 배열 순서와 카메라 좌표계의 y축 방향은 서로 반대입니다. 이를 반영해줍니다.**   
  
$$
    \begin{bmatrix}
        u \\
        -v \\
        -1
    \end{bmatrix}
    =
    \begin{bmatrix}
        1 && 0 && 0 \\
        0 && -1 && 0 \\
        0 && 0 && -1 \\
    \end{bmatrix}
    \begin{bmatrix}
        u \\
        v \\
        1
    \end{bmatrix}
$$  

- **Normalized 좌표계에 정의된 Pixel** $\{u, -v, -1\}$ **을 Pose의 역함수를 통해 World 좌표계** $\{X_w, Y_w, Z_w\}$ **로 변환할 수 있습니다.**   
  
$$
    \begin{bmatrix}
        X_w \\
        Y_w \\
        Z_w
    \end{bmatrix}
    =
    Pose_{cam2world}
    \begin{bmatrix}
        u \\
        -v \\
        -1
    \end{bmatrix}
$$  

$o$, $d$ 는 위 수식으로부터 얻을 수 있습니다.   

$$
[R|T] = Pose_{cam2world}
$$    

$$ 
  o = T
$$

$$
  d = \left\| R * 
  \begin{bmatrix}
        u \\
        -v \\
        -1
    \end{bmatrix} \right\|
$$   

여기서 d는 방향 벡터이므로, normalization을 해줍니다.
#### **2.(a).2. 각 Ray를 여러 포인트로 샘플링**  
다음은 p는 hyper-parameter로써, 저자는 64개, 128개를 사용했습니다.
e.g., 일정 간격으로 0 ~ 63 넘버링 사용  

이를 통해, 위에서 얻은 모든 값을 $R(p) = o+p*d$ 에 대입하여 아래 그림과 같은 샘플 포인트들을 한 이미지로부터 얻을 수 있습니다. 각 포인트의 $(x,y,z)$ 그리고 View direction $(\theta, \phi)$를 만들어 NeRF 모델의 Input으로 사용합니다.   
> NeRF는 보통 100개 이상의 이미지를 사용합니다. 아래 그림에서 느낄 수 있듯이 연산량이 엄청나서 (A100으로 1~2일 소요) 이를 해결하기 위한 후속 논문들이 꽤 나옵니다.  

![](/assets/img/nerf/Picture4.jpg)*출처: https://csm-kr.tistory.com/64*   

### (+) Positional Encoding (PE)  
샘플된 포인트로부터 Position Encoding 적용하는 과정입니다.    


NeRF 모델의 Input으로 샘플된 포인트를 직접 사용하기 보다는 Positional Encoding을 적용한 Feature Vector를 사용했을 때 이미지 내 고주파 성분을 더 잘 표현 가능하여 더 좋은 디테일 결과를 보이기 때문에 이 과정이 추가되었습니다. (*출처: https://csm-kr.tistory.com/64*)   

  
| Positional Encoding 적용 전        | Positional Encoding 적용 후        |
| ---------------------------------- | ---------------------------------- |
| ![](/assets/img/nerf/Picture6.gif) | ![](/assets/img/nerf/Picture8.gif) |
| ![](/assets/img/nerf/Picture7.gif) | ![](/assets/img/nerf/Picture9.gif) |


각 샘플 포인트의 위치 $P = (x,y,z)$ 그리고 View direction $D = (\theta, \phi)$ 총 5개의 파라미터를 fourier poisitonal encoding을 사용하여 각각 feature vector를 만들어줍니다.  
fourier poisitonal encoding 에 관한 자세한 내용은 [[개념 정리] Positional Encoding](_posts/2025-04-08-PositionalEncoding.md) 를 참조 바랍니다.  
$$\gamma_{𝐿} (u_i)=(\sin⁡(2^{0} u_i ),\cos(2^{0} u_i ),\cdots,\sin⁡(2^{𝐿−1} u_i ),\cos (2^{𝐿−1} u_i ))$$  
  
![](/assets/img/nerf/Picture5.jpg)  


> 논문은 $\sin⁡(2^{𝐿−1} \pi u_i )$로 되어있는데 코드 상에서는 $\pi$가 제거되었습니다. 저자는 훈련에 사용된 크기가
> normalized 3D 모델이 [-1, 1] 사이의 위치 값을 가지는 것을 기대했는데, 실제로는 [-1.5, 1.5] 사이의 값을 가져 $\pi$를
> 제거했다고 합니다. ([참고: 저자 Q&A](https://github.com/bmild/nerf/issues/12))    
> 이를 통해 알 수 있는 가장 중요한 포인트는 sin, cos 값이 한 주기를 넘어가지 않도록 Positional Encoding을 설정해야 하는 것입니다.  
  
### (b) Radiance Fields Estimation   
학습 모델을 통해 포인트 컬러와 밀도 $(r,g,b,\sigma)$를 추정하는 과정입니다. 간단한 MLP를 사용했기 때문에 자세한 설명은 생략하겠습니다.  
![](/assets/img/nerf/Picture10.jpg)   
- 초록색 박스 $\gamma(x)$는 Positional Encoding으로부터 나온 Input 값 그리고 60은 한 ray로부터 얻은 샘플 포인트 갯수 입니다.  
- 파란색 박스는 256차원의 Fully Connected Layer를 의미합니다.   
- 빨간색 박스는 Output을 의미합니다.  
  
### (c) Volume Rendering  
Ray가 통과하는 각 샘플 포인트들로부터 누적 계산하는 Volume Rendering 수식을 통해 새로운 Scene을 만들어내는 과정입니다.   
    
$$\sum^{N}_{i=1} T_{i}*(1-\exp(-\sigma_{i}\delta_{i}))*c_i$$    
$$T_{i} = \exp(-\sum^{i-1}_{j=0} \sigma_{i}\delta_{i})$$  
  
각 항의 의미는 다음과 같습니다.    
- $T_i$: Accumulated Transmittance
- $(1-\exp(-\sigma_{i}\delta_{i}))$: Volume Density
- $c_i$: Radiance $\equiv(r,g,b)$
  
여기서, $\delta$는 간 포인트 간의 간격을 의미합니다.   
그리고 각 i에 대하여 Accumulated Transmittance 그리고 Volume Density 결과 경향성은 다음 그래프와 같습니다.
  
| Accumulated Transmittance           | Volume Density                      |
| ----------------------------------- | ----------------------------------- |
| ![](/assets/img/nerf/Picture12.jpg) | ![](/assets/img/nerf/Picture13.jpg) |
  
- Accumulated Transmittance는 x=0일 때 y=1이고 x가 커짐에 따라 y가 점점 0에 수렴하는 단조 감수 함수의 특징을 보입니다.  
- 이와 반대로, Volume Density는 x=0일 때 y=0이고 x가 커짐에 따라 y가 점점 1에 수렴하는 단조 증가 함수 특징을 보입니다.  


그러나 수식만보고 이해가 어렵기 때문에, 예시를 통해 Volume Rendering을 이해 해보겠습니다. (참조: [xoft 블로그](https://xoft.tistory.com/))  

$N=5$, $\delta_i=1$인 어느 한 Ray를 따라 샘플포인트 $u_{i}=\{c_i, \sigma_i\}$가 아래 왼쪽 그림과 같이 있다고 생각해봅시다. 오른쪽 그림은 논문에서 나타낸 왼쪽 그림을 도식화 한 것입니다.   
<figure class="half">
<a href="link"><img src="/assets/img/nerf/Picture11.jpg" align="center" width="50%"></a>
<a href="link"><img src="/assets/img/nerf/Picture14.jpg" align="center" width="50%"></a>
</figure>  

그러면 
- i=1,2일 때  
$T_i$가 1, Volume Density가 0이 되므로 전체 값이 0이 됩니다.

- i=3일 때  
$T_i$가 0~1 사이의 어떤 값을 가지며, Volume Density가 1이 됩니다.

- i=4,5일 때  
$T_i$가 0에 점점 수렴합니다. Volume Density가 0~1 사이의 어떤 값을 가집니다.  

이를 통해, $T_i$가 클수록 빛이 해당 지점을 통과했다는 것을 의미하고 작아질수록 어떤 물체에 부딪침을 알 수 있습니다.  
그리고 Volume Density를 통해 해당 지점에서 물체가 얼만큼 Density를 가지고 있는지, 물체의 특성을 알 수 있습니다.  
   
### (d) Loss  
새로운 Scene에서 추출된 2D 픽셀 컬러 값이 실제 픽셀 컬러 값과 일치 여부를 확인하는 과정입니다.  
Loss 함수는 Coarse-grained 모델의 output $\hat{C}_c(r)$ 그리고 Fine-grained 모델의 output $\hat{C}_f(r)$ 각각 Ground truth $C(r)$와 L2 Loss한 후 선형 결합합니다.  
  
$$L(r) = \|\hat{C}_c(r) - C(r) \|^2_2 + \|\hat{C}_f(r) - C(r) \|^2_2$$
   
여기서, Coarse-grained 모델과 Fine-grained 모델의 차이점은 샘플 포인트 추출 방법에 있습니다.
- Coarse-grained 모델의 output $\hat{C}_c(r)$  
  Ray를 균일한 간격으로 샘플링하여 포인트를 추출합니다.  

- Fine-grained 모델의 output $\hat{C}_f(r)$   
  Volume Rendering으로부터 얻은 각 포인트의 가중치 $w_i = T_{i}*(1-\exp(-\sigma_{i}\delta_{i}))$ 를 통해 물체가 있을 가능성이 높은 지역을 중심으로 Importance Sampling하여 포인트를 추출합니다.   

## Closing
최종 목표는 Gaussian Splatting (GS) 연구이며 GS의 Related work 조사를 위해 NeRFs에 대해 알아봤습니다.  
다음은 NeRFs를 개선하기 위한 후속 논문인 MipNeRFs, Instant-NGP, PlenOctree에 대해 포스트할 계획입니다.
