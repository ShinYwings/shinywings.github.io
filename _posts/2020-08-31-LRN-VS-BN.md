---
title: LRN vs BN
comments: true
tags:
- Batch Normalization
- Local Response Normalization
---

![bn-vs-lrn](/assets/img/BNvsLRN/BN_vs_LRN.png)

| Batch Normalization                                                       | vs   | Local Response Normalization                            |
| ------------------------------------------------------------------------- | ---- | ------------------------------------------------------- |
| $$y= \frac {x-\mu} {\sqrt {\sigma^2 + \epsilon}} \gamma + \beta$$         |      | $$x_i= \frac {x_i} {(k+(\alpha\sum_{j} x_j^2))^\beta}$$ |
| $$\epsilon$$: small constant<br>(+)$$\gamma,\beta$$: learnable parameters |      | (-) $$k, \alpha, \beta$$: hyper-parameters              |
| pixel-wise                                                                |      | pixel-wise                                              |
| (+) 2 trainable parameters<br>    (shift & scale)                         |      | (-) non-trainable parameters                            |
| Internal Covariable Shift                                                 | 중점 | Lateral Inhibition                                      |
| (+) Weight Regularization O                                               |      | (-) Weight Regularization X                             |
| Batch에 대한 정규화                                                       |      | 각각의 Feature position에 대한 정규화                   |
    

- LRN이 더 이상 쓰이지 않는 것에 대한 내 생각
  - hyper-parameters의 수가 너무 많다. 반면에 BN은 trainable parameters가 존재하여 가중치를 업데이트하는데 도움이 된다. 왜냐하면 분포가 일정해지기 때문에 수렴속도가 빠르기 때문이다. 반면에 LRN은 Kernel maps의 분포가 hidden layer를 지나갈 때마다 바뀐다. (internal covariate shift)
  - DL에서 가장 중요한 문제점인 Internal Covariable Shift의 문제를 해결한다.(어떤 논문에서는 이것을 해결하는게 아니지만 여전히 좋은 도구라고 생각 [참조]<https://ml-dnn.tistory.com/6> )    
  - Regularization을 통해 가중치의 급격한 변화를 막는다.
  - Batch에 대한 정규화 작업이 Feature Map에 대한 정규화 작업보다 계산할 parameters의 수가 적다. LRN의 Radius (n) 의 개수에 따라 수십초가 차이난다.(엄청 큼!!)    
  - n = 10일때 보다 n = 5일때 30초가 덜 걸렸다. n = 1 인 BN을 사용했을 때 n = 3보다 40초가 덜 걸렸다. 엄청난 차이를 보인다.
  - LRN의 k는 학습을 불안정하게 만든다. 만약 Input Normalization을 한 상태라면 참조하는 kernel의 합이 1이 안되는데 k = 1 또는 2 를 더해주게 되면 모든 kernel은 비슷한 특징을 보이게 될 것이다. 내 실험에서도 항상 accuracy가 10%를 유지했다. 
