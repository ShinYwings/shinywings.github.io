---
title: Principal Component Analysis (PCA)
comments: true
tags: [PCA]
---
제일 좋은 참조. 내꺼 안봐도 됨    
<https://excelsior-cjh.tistory.com/167>    
<https://ratsgo.github.io/machine%20learning/2017/04/24/PCA/>


![PCA1](/assets/img/PCA/PCA-1.jpg)
![PCA2](/assets/img/PCA/PCA-2.jpg)
![PCA3](/assets/img/PCA/PCA-3.jpg)
![PCA4](/assets/img/PCA/PCA-4.jpg)
![PCA5](/assets/img/PCA/PCA-5.jpg)    
- 왜 분산이 제일 큰 것을 고를까?
  - 분산이 제일 큰 것은 원래 데이터의 설명력을 가장 잘 보존하는 데이터(매개변수) 이기 때문이다. e.g. 3차원의 데이터를 1차원으로 줄일 수 있음         
    
![PCA6](/assets/img/PCA/PCA-6.jpg)
